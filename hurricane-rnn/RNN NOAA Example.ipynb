{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    # Splits data into batches of length n\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = f.read()  \n",
    "        \n",
    "        self.vocab_int = {}\n",
    "        self.int_vocab = {}\n",
    "        \n",
    "        for k, v in enumerate(set(data)):\n",
    "            self.vocab_int[v] = k\n",
    "            self.int_vocab[k] = v\n",
    "            \n",
    "        self.n_classes = len(self.vocab_int)\n",
    "        self.data = np.array([self.vocab_int[t] for t in data])\n",
    "        \n",
    "    def batch(self, batch_size, seq_len):       \n",
    "        char_per_batch = batch_size * seq_len\n",
    "        batch_count = int(len(self.data) / char_per_batch)\n",
    "        \n",
    "        arr = self.data[: char_per_batch * batch_count]\n",
    "        arr = np.reshape(arr, (batch_size, -1))\n",
    "        \n",
    "        for b in range(0, arr.shape[1], seq_len):\n",
    "            x = arr[:, b:b + seq_len]\n",
    "            y = np.zeros_like(x)\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "            yield x, y        \n",
    "            \n",
    "\n",
    "class RNNModel():\n",
    "    \n",
    "    def __init__(self):       \n",
    "        self.data = Dataset('adv.txt')\n",
    "    \n",
    "    def _build_inputs(self):       \n",
    "        self.inputs = tf.placeholder(tf.int32, [self.batch_size, self.seq_len], name='inputs')\n",
    "        self.targets = tf.placeholder(tf.int32, [self.batch_size, self.seq_len], name='targets')      \n",
    "        self.learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "        self.grad_clip = tf.placeholder(tf.float32, name='grad_clip')\n",
    "        \n",
    "    def _build_lstm_layer(self):\n",
    "        def build_lstm_cell():\n",
    "            lstm = tf.contrib.rnn.BasicLSTMCell(self.lstm_size)\n",
    "            lstm_drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = self.keep_prob)\n",
    "            return lstm_drop\n",
    "        \n",
    "        cells = [build_lstm_cell() for _ in range(self.lstm_layers)]\n",
    "        \n",
    "        self.lstm_layers = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        self.initial_state = self.lstm_layers.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        inputs_onehot = tf.one_hot(self.inputs, self.data.n_classes)\n",
    "        self.lstm_output, self.final_state = tf.nn.dynamic_rnn(self.lstm_layers, inputs_onehot, \n",
    "                                                               initial_state=self.initial_state)\n",
    "        \n",
    "    def _build_output(self):\n",
    "        concat = tf.concat(self.lstm_output, axis=1)\n",
    "        x = tf.reshape(concat, [-1, self.lstm_size])\n",
    "        \n",
    "        self.logits = tf.layers.dense(x, self.data.n_classes, kernel_initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "        self.predictions = tf.nn.softmax(self.logits, name='predictions')\n",
    "        \n",
    "    def _build_loss(self):\n",
    "        targets_onehot = tf.one_hot(self.targets, self.data.n_classes)\n",
    "        y_reshaped  = tf.reshape(targets_onehot, self.logits.get_shape())\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=y_reshaped)\n",
    "        self.loss = tf.reduce_mean(loss)     \n",
    "        \n",
    "    def _build_optimizer(self):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), self.grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "        \n",
    "    def get_batches(self):\n",
    "        return self.data.batch(self.batch_size, self.seq_len)\n",
    "        \n",
    "    def build_network(self, batch_size, seq_len, lstm_layers, lstm_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        \n",
    "        # num_layers: number of LSTM layers to use\n",
    "        # lstm_size: number of hidden layers in LSTM cell\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Create the inputs which are one-hot encoded tensors of shape batch_size x seq_steps\n",
    "        self._build_inputs()\n",
    "        \n",
    "        # Create the LSTM layer which contains lstm_size hidden units and is num_layers deep \n",
    "        self._build_lstm_layer()\n",
    "        \n",
    "        # Create the output layers which is just a single dense layer\n",
    "        self._build_output()\n",
    "        \n",
    "        # Create the loss measure\n",
    "        self._build_loss()\n",
    "        \n",
    "        # Create the optimizer\n",
    "        self._build_optimizer()\n",
    "        \n",
    "    def train(self, batch_size, seq_len, lstm_layers, lstm_size, epochs, keep_prob, learning_rate, \n",
    "              grad_clip, save_every_n=200, continue_training=True):\n",
    "        \n",
    "        # Adapted from the Udacity deep learning code\n",
    "        # https://github.com/udacity/deep-learning/blob/master/intro-to-rnns/Anna_KaRNNa.ipynb\n",
    "        \n",
    "        self.build_network(batch_size=batch_size, seq_len=seq_len, lstm_layers=lstm_layers, lstm_size=lstm_size)\n",
    "        saver = tf.train.Saver(max_to_keep=100)\n",
    "        \n",
    "        \n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            if continue_training:\n",
    "                checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "                saver.restore(sess, checkpoint)\n",
    "                counter = int(checkpoint.split('_')[0].replace('checkpoints\\\\i',''))+1\n",
    "            else:\n",
    "                counter = 0\n",
    "            for e in range(epochs):\n",
    "                new_state = sess.run(model.initial_state)\n",
    "\n",
    "                for x, y in model.get_batches():\n",
    "                    start = time.time()\n",
    "                    \n",
    "                    feed = {\n",
    "                        model.inputs: x,\n",
    "                        model.targets: y,\n",
    "                        model.keep_prob: keep_prob,\n",
    "                        model.learning_rate: learning_rate,\n",
    "                        model.initial_state: new_state,\n",
    "                        model.grad_clip: grad_clip\n",
    "                    }\n",
    "                    \n",
    "                    batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                         model.final_state, \n",
    "                                                         model.optimizer], \n",
    "                                                         feed_dict=feed)\n",
    "\n",
    "                    end = time.time()\n",
    "                    \n",
    "                    if counter % 25 == 0:\n",
    "                        print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                              'Training Step: {}... '.format(counter),\n",
    "                              'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                              '{:.4f} sec/batch'.format((end-start)))\n",
    "\n",
    "                    if (counter % save_every_n == 0):\n",
    "                        saver.save(sess, \"checkpoints/i{}_l.ckpt\".format(counter))\n",
    "\n",
    "                    counter += 1\n",
    "            saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "            print('Training complete')\n",
    "            \n",
    "    def predict(self, lstm_layers, lstm_size, num_characters, seed, checkpoint_path=None, top_n=5):\n",
    "        self.build_network(batch_size=1, seq_len=1, lstm_layers=lstm_layers, lstm_size=lstm_size)\n",
    "        samples = [c for c in seed]\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        if checkpoint_path is None:\n",
    "            checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "        else:\n",
    "            checkpoint = checkpoint_path\n",
    "        \n",
    "        def pick_top_n(preds, vocab_size, top_n):\n",
    "            p = np.squeeze(preds)\n",
    "            p[np.argsort(p)[:-top_n]] = 0\n",
    "            p = p / np.sum(p)\n",
    "            c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "            return c\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, checkpoint)\n",
    "            new_state = sess.run(model.initial_state)\n",
    "            \n",
    "            for c in seed:\n",
    "                x = np.zeros((1, 1))\n",
    "                x[0,0] = model.data.vocab_int[c]\n",
    "                \n",
    "                feed = {model.inputs: x,\n",
    "                        model.keep_prob: 1.,\n",
    "                        model.initial_state: new_state}\n",
    "                \n",
    "                preds, new_state = sess.run([model.predictions, model.final_state], \n",
    "                                             feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, model.data.n_classes, top_n)\n",
    "            samples.append(model.data.int_vocab[c])\n",
    "\n",
    "            for i in range(num_characters):\n",
    "                x[0,0] = c\n",
    "                \n",
    "                feed = {model.inputs: x,\n",
    "                        model.keep_prob: 1.,\n",
    "                        model.initial_state: new_state}\n",
    "                \n",
    "                preds, new_state = sess.run([model.predictions, model.final_state], \n",
    "                                             feed_dict=feed)\n",
    "\n",
    "                c = pick_top_n(preds, model.data.n_classes, top_n)\n",
    "                samples.append(model.data.int_vocab[c])\n",
    "\n",
    "        print(''.join(samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNNModel()\n",
    "n_layers = 3\n",
    "internal_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15...  Training Step: 0...  Training loss: 4.3561...  0.3731 sec/batch\n",
      "Epoch: 1/15...  Training Step: 25...  Training loss: 3.2973...  0.2596 sec/batch\n",
      "Epoch: 1/15...  Training Step: 50...  Training loss: 3.1312...  0.2566 sec/batch\n",
      "Epoch: 1/15...  Training Step: 75...  Training loss: 3.0694...  0.2611 sec/batch\n",
      "Epoch: 1/15...  Training Step: 100...  Training loss: 3.0323...  0.2611 sec/batch\n",
      "Epoch: 2/15...  Training Step: 125...  Training loss: 2.9716...  0.2611 sec/batch\n",
      "Epoch: 2/15...  Training Step: 150...  Training loss: 2.9036...  0.2581 sec/batch\n",
      "Epoch: 2/15...  Training Step: 175...  Training loss: 2.7744...  0.2586 sec/batch\n",
      "Epoch: 2/15...  Training Step: 200...  Training loss: 2.7126...  0.2591 sec/batch\n",
      "Epoch: 3/15...  Training Step: 225...  Training loss: 2.5652...  0.2581 sec/batch\n",
      "Epoch: 3/15...  Training Step: 250...  Training loss: 2.4277...  0.2581 sec/batch\n",
      "Epoch: 3/15...  Training Step: 275...  Training loss: 2.2829...  0.2581 sec/batch\n",
      "Epoch: 3/15...  Training Step: 300...  Training loss: 2.1850...  0.2576 sec/batch\n",
      "Epoch: 3/15...  Training Step: 325...  Training loss: 2.0197...  0.2571 sec/batch\n",
      "Epoch: 4/15...  Training Step: 350...  Training loss: 1.8861...  0.2581 sec/batch\n",
      "Epoch: 4/15...  Training Step: 375...  Training loss: 1.7315...  0.2586 sec/batch\n",
      "Epoch: 4/15...  Training Step: 400...  Training loss: 1.5176...  0.2576 sec/batch\n",
      "Epoch: 4/15...  Training Step: 425...  Training loss: 1.4530...  0.2581 sec/batch\n",
      "Epoch: 5/15...  Training Step: 450...  Training loss: 1.4809...  0.2556 sec/batch\n",
      "Epoch: 5/15...  Training Step: 475...  Training loss: 1.3007...  0.2561 sec/batch\n",
      "Epoch: 5/15...  Training Step: 500...  Training loss: 1.1566...  0.2571 sec/batch\n",
      "Epoch: 5/15...  Training Step: 525...  Training loss: 1.1251...  0.2581 sec/batch\n",
      "Epoch: 6/15...  Training Step: 550...  Training loss: 1.1543...  0.2561 sec/batch\n",
      "Epoch: 6/15...  Training Step: 575...  Training loss: 1.0456...  0.2581 sec/batch\n",
      "Epoch: 6/15...  Training Step: 600...  Training loss: 1.0492...  0.2551 sec/batch\n",
      "Epoch: 6/15...  Training Step: 625...  Training loss: 1.0305...  0.2581 sec/batch\n",
      "Epoch: 6/15...  Training Step: 650...  Training loss: 0.9808...  0.2571 sec/batch\n",
      "Epoch: 7/15...  Training Step: 675...  Training loss: 0.8979...  0.2571 sec/batch\n",
      "Epoch: 7/15...  Training Step: 700...  Training loss: 0.9589...  0.2641 sec/batch\n",
      "Epoch: 7/15...  Training Step: 725...  Training loss: 0.8734...  0.2556 sec/batch\n",
      "Epoch: 7/15...  Training Step: 750...  Training loss: 0.9518...  0.2576 sec/batch\n",
      "Epoch: 8/15...  Training Step: 775...  Training loss: 0.8625...  0.2581 sec/batch\n",
      "Epoch: 8/15...  Training Step: 800...  Training loss: 0.8449...  0.2581 sec/batch\n",
      "Epoch: 8/15...  Training Step: 825...  Training loss: 0.8270...  0.2621 sec/batch\n",
      "Epoch: 8/15...  Training Step: 850...  Training loss: 0.7657...  0.2566 sec/batch\n",
      "Epoch: 9/15...  Training Step: 875...  Training loss: 0.8018...  0.2571 sec/batch\n",
      "Epoch: 9/15...  Training Step: 900...  Training loss: 0.7811...  0.2576 sec/batch\n",
      "Epoch: 9/15...  Training Step: 925...  Training loss: 0.7576...  0.2571 sec/batch\n",
      "Epoch: 9/15...  Training Step: 950...  Training loss: 0.6808...  0.2561 sec/batch\n",
      "Epoch: 9/15...  Training Step: 975...  Training loss: 0.6943...  0.2551 sec/batch\n",
      "Epoch: 10/15...  Training Step: 1000...  Training loss: 0.7539...  0.2581 sec/batch\n",
      "Epoch: 10/15...  Training Step: 1025...  Training loss: 0.6828...  0.2561 sec/batch\n",
      "Epoch: 10/15...  Training Step: 1050...  Training loss: 0.6202...  0.2576 sec/batch\n",
      "Epoch: 10/15...  Training Step: 1075...  Training loss: 0.7201...  0.2596 sec/batch\n",
      "Epoch: 11/15...  Training Step: 1100...  Training loss: 0.6539...  0.2561 sec/batch\n",
      "Epoch: 11/15...  Training Step: 1125...  Training loss: 0.6879...  0.2561 sec/batch\n",
      "Epoch: 11/15...  Training Step: 1150...  Training loss: 0.6444...  0.2566 sec/batch\n",
      "Epoch: 11/15...  Training Step: 1175...  Training loss: 0.6383...  0.2566 sec/batch\n",
      "Epoch: 12/15...  Training Step: 1200...  Training loss: 0.6734...  0.2581 sec/batch\n",
      "Epoch: 12/15...  Training Step: 1225...  Training loss: 0.6399...  0.2556 sec/batch\n",
      "Epoch: 12/15...  Training Step: 1250...  Training loss: 0.6589...  0.2561 sec/batch\n",
      "Epoch: 12/15...  Training Step: 1275...  Training loss: 0.5813...  0.2581 sec/batch\n",
      "Epoch: 12/15...  Training Step: 1300...  Training loss: 0.6022...  0.2551 sec/batch\n",
      "Epoch: 13/15...  Training Step: 1325...  Training loss: 0.5968...  0.2556 sec/batch\n",
      "Epoch: 13/15...  Training Step: 1350...  Training loss: 0.6364...  0.2551 sec/batch\n",
      "Epoch: 13/15...  Training Step: 1375...  Training loss: 0.6216...  0.2551 sec/batch\n",
      "Epoch: 13/15...  Training Step: 1400...  Training loss: 0.6268...  0.2601 sec/batch\n",
      "Epoch: 14/15...  Training Step: 1425...  Training loss: 0.6136...  0.2601 sec/batch\n",
      "Epoch: 14/15...  Training Step: 1450...  Training loss: 0.5684...  0.2556 sec/batch\n",
      "Epoch: 14/15...  Training Step: 1475...  Training loss: 0.5713...  0.2556 sec/batch\n",
      "Epoch: 14/15...  Training Step: 1500...  Training loss: 0.5762...  0.2576 sec/batch\n",
      "Epoch: 14/15...  Training Step: 1525...  Training loss: 0.5745...  0.2611 sec/batch\n",
      "Epoch: 15/15...  Training Step: 1550...  Training loss: 0.5526...  0.2576 sec/batch\n",
      "Epoch: 15/15...  Training Step: 1575...  Training loss: 0.5784...  0.2581 sec/batch\n",
      "Epoch: 15/15...  Training Step: 1600...  Training loss: 0.5279...  0.2566 sec/batch\n",
      "Epoch: 15/15...  Training Step: 1625...  Training loss: 0.5735...  0.2581 sec/batch\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "model.train(batch_size=256, \n",
    "            seq_len=45, \n",
    "            lstm_layers=n_layers, \n",
    "            lstm_size=internal_size, \n",
    "            epochs=15, \n",
    "            keep_prob=0.5, \n",
    "            learning_rate=0.001, \n",
    "            grad_clip=5,\n",
    "            continue_training=False\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 'ZCZC MIATCDAT1 ALL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i200_l.ckpt\n",
      "ZCZC MIATCDAT1 ALLTUAN. .N\n",
      "CCN EIIICTM LALTTLO LRIRURAO .910 0MPEH...2595 HMHN..I\n",
      "\n",
      " N ARRMRE E SAFTRN  INDDRRONNEEUN   S SRRCECET  TITCHLLEASOSEO\n",
      "EDERNIOTOE TA TROEOTOSN. AOAOSS.\n",
      "OHTI AERTDSSE EO OATERRARTEO I   ITER O SAAA ELONAIO AR  TUAIEENO T\n",
      " OSAISRILIOIE DA EREIAISI.  IASO UNU O TOAI ACSSCTO N SEIR ANCTHTS. NOANS AAAIA ANIHNERN TI HETAT RST AIE IAIA I RTRMA A ORNAOLAEC CN I CRR AIFE  TSRRLOA ONS\n",
      "\n",
      "NCRCCOESDT.A.TOOR  HOTR OOTACSSEASEE TA INUAHROAHOAEATNA. I.RA  TE LOOLIIECRASE   AEDTEIIAIRS TRO NATH ISR TINC E\n"
     ]
    }
   ],
   "source": [
    "model.predict(\n",
    "            lstm_layers=n_layers, \n",
    "            lstm_size=internal_size,\n",
    "            num_characters=500, \n",
    "            seed=seed,\n",
    "            checkpoint_path='checkpoints/i200_l.ckpt',\n",
    "            top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i1635_l512.ckpt\n",
      "ZCZC MIATCDAT1 ALL\n",
      "TTAA00 KNHC DDHHMM\n",
      " \n",
      "BULLETIN\n",
      "TROPICAL STORM FAY ADVISORY NUMBER  14\n",
      "NWS TPC/NATIONAL HURRICANE CENTER MIAMI FL   AL172006\n",
      "500 PM EDT TUE AUG 29 2016\n",
      " \n",
      "...DEPRESTION CONTINUES MOVING NEWAR OR WEST-NORTHWESTWARD...\n",
      " \n",
      " \n",
      "SUMMARY OF 1100 PM AST...0300 UTC...INFORMATION\n",
      "------------------------------------------------\n",
      "LOCATION...28.5N 69.7W\n",
      "ABOUT 100 MI...195 KM SE OF THE LESWARD ISLANDS\n",
      "MAXIMUM SUSTAINED WINDS...35 MPH...65 KM/H\n",
      "PRESENT MOVEMENT...WNW OR 295 DEGREES AT 10 MPH...20 KM/H\n",
      "MINIMUM CENTRAL PRESSURE...1006 MB...29.77 INCHES\n",
      " \n",
      " \n",
      "WATCHES AND WARNINGS\n",
      "---------------------\n",
      "CHANGES WITH THIS ADVISORY...\n",
      " \n",
      "NONE.\n",
      " \n",
      "SUMMARY OF WATCHES AND WARNINGS IN EFFECT...\n",
      " \n",
      "A TROPICAL STORM WARNING IS IN EFFECT FOR...\n",
      " \n",
      "A HURRICANE WARNING MEANS THAT HURRICANE CONDITIONS ARE EXPECTED\n",
      "WITHIN THE WARNING AREA WITHIN THE\n",
      "NEXT 24 HOURS.\n",
      "\n",
      "FOR STORM INFORMATION SPECIFIC TO YOUR AREA...PLEASE MONITOR\n",
      "PRODUCTS ISSUEC BY YOUR NATIONAL\n",
      "METEOROLOGICAL SERVICE.\n",
      " \n",
      " \n",
      "DISCUSSION AND 48-HOUR OUTLOOK\n",
      "------------------------------\n",
      "AT 1100 PM EDT...0300 UTC...THE CENTER OF HURRICANE GALON WAS LOCATED\n",
      "NEAR LATITUDE 14.1 NORTH...LONGITUDE 65.3 WEST.  THE DEPRESSION IS TO THE WASN INCREASE IN FORWARD SPEED.  ON THE FORECAST TRACK...THE CENTER OF TROPICAL STORM WARNING FOR THE\n",
      "COAST OF THE CENTER MOVES OVER\n",
      "PORTIONS OF THE\n",
      "COAST OF MEXICO...WITH POSSIBLE ISOLATED MAXIMUM AMOUNTS OF 10 INCHES POSSIBLE.  A TROPICAL\n",
      "STORM WATCH MEANS THAT TROPICAL STORM CONDITIONS ARE\n",
      "POSSIBLE WITHIN THE WATCH AREA...IN THIS CASE WITHIN THE NEXT 24 HOURS.\n",
      " \n",
      "FOR STORM INFORMATION SPECIFIC TO YOUR AREA...PLEASE MONITOR\n",
      "PRODUCTS ISSUED BY YOUR NATIONAL METEOROLOGICAL SERVICE.\n",
      "\n",
      "\n",
      "DISCUSSION AND 48-HOUR OUTLOOK\n",
      "-------------------------------\n",
      "AT 1100 PM AST.-.0300 UTC...THE CENTER OF HURRICANE GASTON WAS\n",
      "LOCATED NEAR LATITUDE 24.5 NORTH...LONGITUDE 69.3 WEST. THE\n",
      "DEPRESSION IS MOVING TOWARD THE NORTHWEST NEAR 10 MPH...29\n",
      "KM/HR...AND\n",
      "THIS MOTION IS EXPECTED TO CANTINUE TODAY.  ON THE FORECAST TRACK...THE\n",
      "CENTER OF THE DEPRESSION IS\n",
      "EXPECTED TO BECOME A HARRICANE WATERS THE NEW ENGLAND AND THE SOUTHEASTERN BAHAMAS TONIGHT.\n",
      " \n",
      "MAXIMUM SUSTAINED WINDS ARE NEAR 30 MPH...55 KM/HR...WITH HIGHER\n",
      "GUSTS.  SOME STRENGTHENING IS POSSIBLE DURING THE NEXT 28 HOURS.\n",
      " \n",
      "MAXIMUM SUSTAINED WINDS ARE NEAR 45 MPH...65 KM/H...WITH HIGHER\n",
      "GUSTS.  SOME SLOW STRENGTHENING IS POSSIBLE DURING\n",
      "THE NEXT 24 HOURS.\n",
      "\n",
      "TROPICAL STORM FORCE WINDS EXTEND OUTWARD UP TO 200 MILES...325 KM.\n",
      " \n",
      "THE ESTIMATED MINIMUM CENTRAL PRESSURE IS 999 MB...29.75 INCHES.\n",
      "  \n",
      " \n",
      "HAZARDS AFFECTING LAND\n",
      "-----------------------\n",
      "NONE.\n",
      " \n",
      " \n",
      "NEXT ADVISORY\n",
      "-------------\n",
      "NEXT COMPLETE ADVISORY...500 AM AST.\n",
      " \n",
      "$$\n",
      "FORECASTER BRONN\n",
      " \n",
      "NNNN\n",
      "ZCZC MIATCPAT4 ALL\n",
      "TTAA00 KNHC DDHHMM\n",
      "BULLETIN\n",
      "TROPICAL STORM DANNY ADVISORY NUMBER  14\n",
      "NWS NATIONAL HURRICANE CENTER MIAMI FL       AL042013\n",
      "1100 PM AST THU SEP 11 2016\n",
      "\n",
      "...DEPRESSION CONTINUES MOVING NORTHWESTWARD...\n",
      "\n",
      "\n",
      "SUMMARY OF 1100 AM AST...1500 UTC...INFORMATION\n",
      "-----------------------------------------------\n",
      "LOCATION...18.5N 77.7W\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "model.predict(\n",
    "            lstm_layers=n_layers, \n",
    "            lstm_size=internal_size,\n",
    "            num_characters=3000, \n",
    "            seed=seed,\n",
    "            checkpoint_path='checkpoints/i1635_l512.ckpt',\n",
    "            top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
